{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pommerman Demo.\n",
    "\n",
    "This notebook demonstrates how to train Pommerman agents. Please let us know at support@pommerman.com if you run into any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error GL! You will not be able to render --> Library \"GLU\" not found.\n",
      "['AdvancedLesson-v0', 'PommeFFACompetition-v0', 'PommeFFACompetitionFast-v0', 'PommeFFAFast-v0', 'PommeFFA-v1', 'PommeFFAFast-v3', 'PommeFFAFast-v4', 'Lesson1-v0', 'Lesson2-v0', 'Lesson2b-v0', 'Lesson2c-v0', 'Lesson2d-v0', 'Lesson2e-v0', 'Lesson3-v0', 'Lesson3b-v0', 'Lesson3c-v0', 'Lesson3d-v0', 'OneVsOne-v0', 'PommeRadioCompetition-v2', 'PommeRadio-v2', 'Simple-v0', 'SimpleRandomTeam-v0', 'SimpleTeam-v0', 'PommeTeamCompetition-v0', 'PommeTeamCompetitionFast-v0', 'PommeTeamCompetition-v1', 'PommeTeam-v0', 'PommeTeamFast-v0', 'PommeTeamSimple-v0', 'tournament-v0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pommerman\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme as Pomme_v0\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility\n",
    "from pommerman import agents\n",
    "from pommerman import envs\n",
    "from pommerman import constants\n",
    "from pommerman import characters\n",
    "\n",
    "# print all env configs\n",
    "print(pommerman.REGISTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with stable baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inherit pommerman env and make it compatible with stable-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPomme(Pomme_v0):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.obs_raw = None # store the raw version of observation\n",
    "        self.training_idx = 1 # idx of the agent being trained\n",
    "    \n",
    "    # function to flatten pommerman observation\n",
    "    def _transform_obs(self, obs_raw):\n",
    "        obs_training = obs_raw[self.training_idx] # default the first agent to be trained\n",
    "\n",
    "        # construct flattened observation\n",
    "        obs = [\n",
    "            *np.array(obs_training[\"board\"]).reshape(-1),\n",
    "            *np.array(obs_training[\"bomb_blast_strength\"]).reshape(-1),\n",
    "            *np.array(obs_training[\"bomb_life\"]).reshape(-1),\n",
    "            *np.array(obs_training[\"position\"]).reshape(-1),\n",
    "            obs_training[\"ammo\"],\n",
    "            obs_training[\"blast_strength\"],\n",
    "            obs_training[\"can_kick\"],\n",
    "            obs_training[\"teammate\"].value,\n",
    "            obs_training[\"enemies\"][0].value,\n",
    "            \n",
    "            # uncommon if training 1 v 1\n",
    "            obs_training[\"enemies\"][0].value,\n",
    "            obs_training[\"enemies\"][0].value,\n",
    "            \n",
    "            # uncommon if training 2 v 2\n",
    "#             obs_training[\"enemies\"][1].value,\n",
    "#             obs_training[\"enemies\"][2].value,\n",
    "        ]\n",
    "        return obs\n",
    "    \n",
    "    def get_obs_raw(self):\n",
    "        return self.obs_raw\n",
    "\n",
    "    def step(self, action_training):\n",
    "        action_nontraining = self.act(self.obs_raw)\n",
    "        actions = [*action_nontraining, action_training]\n",
    "        obs_raw, reward, done, info = super().step(actions)\n",
    "        self.obs_raw = obs_raw\n",
    "        return self._transform_obs(obs_raw), reward[self.training_idx], done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        obs_raw = super().reset()\n",
    "        self.obs_raw = obs_raw\n",
    "        return self._transform_obs(obs_raw)\n",
    "    \n",
    "    def render(self,\n",
    "               mode=None,\n",
    "               close=False,\n",
    "               record_pngs_dir=None,\n",
    "               record_json_dir=None,\n",
    "               do_sleep=True):\n",
    "        super().render(mode=mode,\n",
    "                       close=close,\n",
    "                       record_pngs_dir=record_pngs_dir,\n",
    "                       record_json_dir=record_json_dir,\n",
    "                       do_sleep=do_sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_v3_fast_env():\n",
    "    \"\"\"Start up a FFA config with the default settings.\"\"\"\n",
    "    env = CustomPomme\n",
    "    game_type = constants.GameType.Team\n",
    "    env_entry_point = 'CustomPomme'\n",
    "    env_id = 'PommeTeamFast-v3'\n",
    "    env_kwargs = {\n",
    "        'game_type': game_type,\n",
    "        'board_size': 8,\n",
    "        'num_rigid': 0,\n",
    "        'num_wood': 0,\n",
    "        'num_items': 0,\n",
    "        'max_steps': constants.MAX_STEPS,\n",
    "        'render_fps': 1000,\n",
    "        'env': env_entry_point,\n",
    "    }\n",
    "    agent = characters.Bomber\n",
    "    return locals()\n",
    "\n",
    "def one_vs_one_v3_env():\n",
    "    \"\"\"Start up a FFA config with the default settings.\"\"\"\n",
    "    env = CustomPomme\n",
    "    game_type = constants.GameType.OneVsOne\n",
    "    env_entry_point = 'CustomPomme'\n",
    "    env_id = 'PommeOneVsOneFast-v3'\n",
    "    env_kwargs = {\n",
    "        'game_type': game_type,\n",
    "        'board_size': 8,\n",
    "        'num_rigid': 0,\n",
    "        'num_wood': 0,\n",
    "        'num_items': 0,\n",
    "        'max_steps': constants.MAX_STEPS,\n",
    "        'render_fps': 1000,\n",
    "        'env': env_entry_point,\n",
    "    }\n",
    "    agent = characters.Bomber\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "\n",
    "config = one_vs_one_v3_env()\n",
    "env_pom = CustomPomme(**config[\"env_kwargs\"])\n",
    "\n",
    "# config agents\n",
    "agents = []\n",
    "\n",
    "# Add simple agents\n",
    "for agent_id in range(1):\n",
    "    agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "    \n",
    "# add player agent(to train)\n",
    "agents.append(PlayerAgent(config[\"agent\"](1, config[\"game_type\"])))\n",
    "\n",
    "env_pom.set_agents(agents)\n",
    "env_pom.set_training_agent(agents[1].agent_id)\n",
    "env_pom.set_init_game_state(None)\n",
    "\n",
    "# Seed and reset the environment\n",
    "env_pom.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log function during training, implement if needed\n",
    "def log(local_var, global_var):\n",
    "    pass\n",
    "#     display(local_var)\n",
    "#     display(global_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_cpu = 1\n",
    "env = DummyVecEnv([lambda: env_pom for i in range(n_cpu)])\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=1, \n",
    "             n_steps = 3000, # batch_size = n_step * num_env\n",
    "             ent_coef = 0.001, # entropy coefficient\n",
    "             tensorboard_log=\"./ppo_pommerman_tensorboard/\")\n",
    "model = model.learn(total_timesteps=5000000, # num_update = total_timesteps // batch_size\n",
    "                    callback = log)\n",
    "model.save(\"ppo2_pommerman_500000_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "Episode 0 finished\n",
      "Episode 1 finished\n",
      "Episode 2 finished\n",
      "Episode 3 finished\n",
      "Episode 4 finished\n",
      "Episode 5 finished\n",
      "Episode 6 finished\n",
      "Episode 7 finished\n",
      "Episode 8 finished\n",
      "Episode 9 finished\n",
      "Win  10 / 10  games\n",
      "Tie  0 / 10  games\n",
      "Lose  0 / 10  games\n"
     ]
    }
   ],
   "source": [
    "# del model # remove to demonstrate saving and loading\n",
    "model = PPO2.load(\"ppo2_pommerman_500000_2\")\n",
    "\n",
    "n_cpu = 1\n",
    "env = DummyVecEnv([lambda: env_pom for i in range(n_cpu)])\n",
    "model.envs = env\n",
    "\n",
    "# test the learned model\n",
    "num_win = 0\n",
    "num_tie = 0\n",
    "num_lose = 0\n",
    "total = 10 # number of playouts\n",
    "for i_episode in range(total):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    info = None\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action_training, _states = model.predict(obs)\n",
    "#         print(action_training)\n",
    "        obs, rewards, dones, infos = env.step(action_training)\n",
    "#         print(infos)\n",
    "        done = dones[0]\n",
    "        info = infos[0]\n",
    "        time.sleep(0.1)\n",
    "    print('Episode {} finished'.format(i_episode))\n",
    "    if(info[\"result\"].value == 0):\n",
    "        if(1 in info[\"winners\"]):\n",
    "            num_win+=1\n",
    "        else:\n",
    "            num_lose+=1\n",
    "    elif(info[\"result\"].value == 2):\n",
    "        num_tie+=1\n",
    "#     print(info)\n",
    "env.close()\n",
    "print(\"Win \", num_win, \"/\", total, \" games\")\n",
    "print(\"Tie \", num_tie, \"/\", total, \" games\")\n",
    "print(\"Lose \", num_lose, \"/\", total, \" games\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "# multiprocess environment\n",
    "n_cpu = 1\n",
    "env = DummyVecEnv([lambda: gym.make('CartPole-v1') for i in range(n_cpu)])\n",
    "\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo2_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO2.load(\"ppo2_cartpole\")\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs)\n",
    "print(env.buf_obs[None].shape)\n",
    "print(env.observation_space)\n",
    "\n",
    "action, _states = model.predict(obs)\n",
    "obs, rewards, dones, info = env.step(action)\n",
    "\n",
    "print(obs)\n",
    "print(rewards)\n",
    "print(dones)\n",
    "print(info)\n",
    "\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = env.step(action)\n",
    "#     env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
